{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89af832-0040-4ee3-b1d2-3f5ce77cceaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0471b2825f1842e19988d6bc95cf468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.2649 | train_acc: 0.9102 | test_loss: 0.1271 | test_acc: 0.9630\n",
      "Epoch: 2 | train_loss: 0.1277 | train_acc: 0.9631 | test_loss: 0.0878 | test_acc: 0.9803\n",
      "Epoch: 3 | train_loss: 0.0957 | train_acc: 0.9760 | test_loss: 0.0708 | test_acc: 0.9873\n",
      "Epoch: 4 | train_loss: 0.0793 | train_acc: 0.9791 | test_loss: 0.0556 | test_acc: 0.9942\n",
      "Epoch: 5 | train_loss: 0.0705 | train_acc: 0.9817 | test_loss: 0.0485 | test_acc: 0.9954\n",
      "Epoch: 6 | train_loss: 0.0622 | train_acc: 0.9844 | test_loss: 0.0393 | test_acc: 0.9942\n",
      "Epoch: 7 | train_loss: 0.0567 | train_acc: 0.9864 | test_loss: 0.0382 | test_acc: 0.9954\n",
      "Epoch: 8 | train_loss: 0.0539 | train_acc: 0.9864 | test_loss: 0.0332 | test_acc: 0.9954\n",
      "Epoch: 9 | train_loss: 0.0525 | train_acc: 0.9867 | test_loss: 0.0338 | test_acc: 0.9954\n",
      "Epoch: 10 | train_loss: 0.0464 | train_acc: 0.9887 | test_loss: 0.0347 | test_acc: 0.9942\n",
      "New best validation accuracy: 0.9954\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707e9b4f79464f78b426e7314365693f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.2649 | train_acc: 0.9102 | test_loss: 0.1271 | test_acc: 0.9630\n",
      "Epoch: 2 | train_loss: 0.1277 | train_acc: 0.9631 | test_loss: 0.0878 | test_acc: 0.9803\n",
      "Epoch: 3 | train_loss: 0.0957 | train_acc: 0.9760 | test_loss: 0.0708 | test_acc: 0.9873\n",
      "Epoch: 4 | train_loss: 0.0793 | train_acc: 0.9791 | test_loss: 0.0557 | test_acc: 0.9942\n",
      "Epoch: 5 | train_loss: 0.0706 | train_acc: 0.9817 | test_loss: 0.0486 | test_acc: 0.9954\n",
      "Epoch: 6 | train_loss: 0.0623 | train_acc: 0.9844 | test_loss: 0.0393 | test_acc: 0.9942\n",
      "Epoch: 7 | train_loss: 0.0568 | train_acc: 0.9864 | test_loss: 0.0383 | test_acc: 0.9954\n",
      "Epoch: 8 | train_loss: 0.0539 | train_acc: 0.9864 | test_loss: 0.0333 | test_acc: 0.9954\n",
      "Epoch: 9 | train_loss: 0.0526 | train_acc: 0.9867 | test_loss: 0.0340 | test_acc: 0.9954\n",
      "Epoch: 10 | train_loss: 0.0465 | train_acc: 0.9887 | test_loss: 0.0348 | test_acc: 0.9942\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 0}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7087874b7e45849c6e35ec6a6e24a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.3571 | train_acc: 0.8590 | test_loss: 0.1883 | test_acc: 0.9398\n",
      "Epoch: 2 | train_loss: 0.1884 | train_acc: 0.9378 | test_loss: 0.1428 | test_acc: 0.9630\n",
      "Epoch: 3 | train_loss: 0.1535 | train_acc: 0.9555 | test_loss: 0.1213 | test_acc: 0.9769\n",
      "Epoch: 4 | train_loss: 0.1338 | train_acc: 0.9631 | test_loss: 0.1058 | test_acc: 0.9803\n",
      "Epoch: 5 | train_loss: 0.1216 | train_acc: 0.9693 | test_loss: 0.1000 | test_acc: 0.9780\n",
      "Epoch: 6 | train_loss: 0.1113 | train_acc: 0.9731 | test_loss: 0.0896 | test_acc: 0.9838\n",
      "Epoch: 7 | train_loss: 0.1028 | train_acc: 0.9761 | test_loss: 0.0842 | test_acc: 0.9803\n",
      "Epoch: 8 | train_loss: 0.1003 | train_acc: 0.9735 | test_loss: 0.0789 | test_acc: 0.9850\n",
      "Epoch: 9 | train_loss: 0.0955 | train_acc: 0.9753 | test_loss: 0.0732 | test_acc: 0.9850\n",
      "Epoch: 10 | train_loss: 0.0895 | train_acc: 0.9791 | test_loss: 0.0748 | test_acc: 0.9884\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7c988110b4463c8b07a419484a0933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.3571 | train_acc: 0.8590 | test_loss: 0.1883 | test_acc: 0.9398\n",
      "Epoch: 2 | train_loss: 0.1884 | train_acc: 0.9378 | test_loss: 0.1428 | test_acc: 0.9630\n",
      "Epoch: 3 | train_loss: 0.1535 | train_acc: 0.9555 | test_loss: 0.1214 | test_acc: 0.9769\n",
      "Epoch: 4 | train_loss: 0.1338 | train_acc: 0.9631 | test_loss: 0.1058 | test_acc: 0.9803\n",
      "Epoch: 5 | train_loss: 0.1216 | train_acc: 0.9693 | test_loss: 0.1001 | test_acc: 0.9780\n",
      "Epoch: 6 | train_loss: 0.1113 | train_acc: 0.9731 | test_loss: 0.0896 | test_acc: 0.9838\n",
      "Epoch: 7 | train_loss: 0.1029 | train_acc: 0.9761 | test_loss: 0.0843 | test_acc: 0.9803\n",
      "Epoch: 8 | train_loss: 0.1003 | train_acc: 0.9735 | test_loss: 0.0789 | test_acc: 0.9850\n",
      "Epoch: 9 | train_loss: 0.0955 | train_acc: 0.9753 | test_loss: 0.0733 | test_acc: 0.9850\n",
      "Epoch: 10 | train_loss: 0.0895 | train_acc: 0.9791 | test_loss: 0.0748 | test_acc: 0.9884\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9344976dcf1b46b5a108a9fdf923c621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.3414 | train_acc: 0.8758 | test_loss: 0.1697 | test_acc: 0.9386\n",
      "Epoch: 2 | train_loss: 0.1634 | train_acc: 0.9496 | test_loss: 0.1158 | test_acc: 0.9688\n",
      "Epoch: 3 | train_loss: 0.1229 | train_acc: 0.9735 | test_loss: 0.0907 | test_acc: 0.9844\n",
      "Epoch: 4 | train_loss: 0.1024 | train_acc: 0.9743 | test_loss: 0.0768 | test_acc: 0.9855\n",
      "Epoch: 5 | train_loss: 0.0894 | train_acc: 0.9772 | test_loss: 0.0671 | test_acc: 0.9900\n",
      "Epoch: 6 | train_loss: 0.0837 | train_acc: 0.9791 | test_loss: 0.0564 | test_acc: 0.9911\n",
      "Epoch: 7 | train_loss: 0.0735 | train_acc: 0.9837 | test_loss: 0.0502 | test_acc: 0.9922\n",
      "Epoch: 8 | train_loss: 0.0669 | train_acc: 0.9854 | test_loss: 0.0470 | test_acc: 0.9911\n",
      "Epoch: 9 | train_loss: 0.0633 | train_acc: 0.9854 | test_loss: 0.0416 | test_acc: 0.9955\n",
      "Epoch: 10 | train_loss: 0.0609 | train_acc: 0.9850 | test_loss: 0.0456 | test_acc: 0.9944\n",
      "New best validation accuracy: 0.9955\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9895455febe24271a948ef337afe9fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.3414 | train_acc: 0.8758 | test_loss: 0.1697 | test_acc: 0.9386\n",
      "Epoch: 2 | train_loss: 0.1634 | train_acc: 0.9496 | test_loss: 0.1158 | test_acc: 0.9688\n",
      "Epoch: 3 | train_loss: 0.1229 | train_acc: 0.9735 | test_loss: 0.0907 | test_acc: 0.9844\n",
      "Epoch: 4 | train_loss: 0.1024 | train_acc: 0.9743 | test_loss: 0.0768 | test_acc: 0.9855\n",
      "Epoch: 5 | train_loss: 0.0894 | train_acc: 0.9772 | test_loss: 0.0671 | test_acc: 0.9900\n",
      "Epoch: 6 | train_loss: 0.0838 | train_acc: 0.9791 | test_loss: 0.0564 | test_acc: 0.9911\n",
      "Epoch: 7 | train_loss: 0.0735 | train_acc: 0.9837 | test_loss: 0.0502 | test_acc: 0.9922\n",
      "Epoch: 8 | train_loss: 0.0670 | train_acc: 0.9854 | test_loss: 0.0471 | test_acc: 0.9911\n",
      "Epoch: 9 | train_loss: 0.0634 | train_acc: 0.9854 | test_loss: 0.0417 | test_acc: 0.9955\n",
      "Epoch: 10 | train_loss: 0.0610 | train_acc: 0.9850 | test_loss: 0.0457 | test_acc: 0.9944\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 64, 'optimizer': 'SGD', 'weight_decay': 0}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e6ce507fa54e57bc4cd885d1fd3aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.4694 | train_acc: 0.8052 | test_loss: 0.2602 | test_acc: 0.9252\n",
      "Epoch: 2 | train_loss: 0.2419 | train_acc: 0.9190 | test_loss: 0.1859 | test_acc: 0.9431\n",
      "Epoch: 3 | train_loss: 0.1971 | train_acc: 0.9372 | test_loss: 0.1576 | test_acc: 0.9487\n",
      "Epoch: 4 | train_loss: 0.1748 | train_acc: 0.9449 | test_loss: 0.1391 | test_acc: 0.9643\n",
      "Epoch: 5 | train_loss: 0.1592 | train_acc: 0.9512 | test_loss: 0.1268 | test_acc: 0.9654\n",
      "Epoch: 6 | train_loss: 0.1480 | train_acc: 0.9564 | test_loss: 0.1192 | test_acc: 0.9688\n",
      "Epoch: 7 | train_loss: 0.1367 | train_acc: 0.9631 | test_loss: 0.1102 | test_acc: 0.9810\n",
      "Epoch: 8 | train_loss: 0.1298 | train_acc: 0.9716 | test_loss: 0.1039 | test_acc: 0.9833\n",
      "Epoch: 9 | train_loss: 0.1230 | train_acc: 0.9738 | test_loss: 0.0994 | test_acc: 0.9844\n",
      "Epoch: 10 | train_loss: 0.1179 | train_acc: 0.9742 | test_loss: 0.0955 | test_acc: 0.9799\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.001, 'batch_size': 64, 'optimizer': 'SGD', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5fa962bcdd4f58b35e247692b315fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.4694 | train_acc: 0.8052 | test_loss: 0.2602 | test_acc: 0.9252\n",
      "Epoch: 2 | train_loss: 0.2419 | train_acc: 0.9190 | test_loss: 0.1859 | test_acc: 0.9431\n",
      "Epoch: 3 | train_loss: 0.1972 | train_acc: 0.9372 | test_loss: 0.1576 | test_acc: 0.9487\n",
      "Epoch: 4 | train_loss: 0.1748 | train_acc: 0.9449 | test_loss: 0.1391 | test_acc: 0.9643\n",
      "Epoch: 5 | train_loss: 0.1592 | train_acc: 0.9512 | test_loss: 0.1268 | test_acc: 0.9654\n",
      "Epoch: 6 | train_loss: 0.1480 | train_acc: 0.9564 | test_loss: 0.1192 | test_acc: 0.9688\n",
      "Epoch: 7 | train_loss: 0.1367 | train_acc: 0.9631 | test_loss: 0.1102 | test_acc: 0.9810\n",
      "Epoch: 8 | train_loss: 0.1298 | train_acc: 0.9716 | test_loss: 0.1039 | test_acc: 0.9833\n",
      "Epoch: 9 | train_loss: 0.1230 | train_acc: 0.9738 | test_loss: 0.0994 | test_acc: 0.9844\n",
      "Epoch: 10 | train_loss: 0.1179 | train_acc: 0.9742 | test_loss: 0.0955 | test_acc: 0.9799\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a490762d74d4483a9ce9b5048e9d0129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6001 | train_acc: 0.7683 | test_loss: 0.4659 | test_acc: 0.9039\n",
      "Epoch: 2 | train_loss: 0.4086 | train_acc: 0.9082 | test_loss: 0.3378 | test_acc: 0.9236\n",
      "Epoch: 3 | train_loss: 0.3181 | train_acc: 0.9172 | test_loss: 0.2695 | test_acc: 0.9282\n",
      "Epoch: 4 | train_loss: 0.2667 | train_acc: 0.9234 | test_loss: 0.2278 | test_acc: 0.9352\n",
      "Epoch: 5 | train_loss: 0.2333 | train_acc: 0.9287 | test_loss: 0.1995 | test_acc: 0.9433\n",
      "Epoch: 6 | train_loss: 0.2086 | train_acc: 0.9385 | test_loss: 0.1799 | test_acc: 0.9630\n",
      "Epoch: 7 | train_loss: 0.1895 | train_acc: 0.9432 | test_loss: 0.1631 | test_acc: 0.9641\n",
      "Epoch: 8 | train_loss: 0.1788 | train_acc: 0.9492 | test_loss: 0.1493 | test_acc: 0.9641\n",
      "Epoch: 9 | train_loss: 0.1648 | train_acc: 0.9553 | test_loss: 0.1390 | test_acc: 0.9757\n",
      "Epoch: 10 | train_loss: 0.1535 | train_acc: 0.9618 | test_loss: 0.1311 | test_acc: 0.9699\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ff0f6ae5f34cc2b4a797fa4b219af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6001 | train_acc: 0.7683 | test_loss: 0.4659 | test_acc: 0.9039\n",
      "Epoch: 2 | train_loss: 0.4086 | train_acc: 0.9082 | test_loss: 0.3378 | test_acc: 0.9236\n",
      "Epoch: 3 | train_loss: 0.3181 | train_acc: 0.9172 | test_loss: 0.2695 | test_acc: 0.9282\n",
      "Epoch: 4 | train_loss: 0.2667 | train_acc: 0.9234 | test_loss: 0.2278 | test_acc: 0.9352\n",
      "Epoch: 5 | train_loss: 0.2334 | train_acc: 0.9287 | test_loss: 0.1995 | test_acc: 0.9433\n",
      "Epoch: 6 | train_loss: 0.2086 | train_acc: 0.9385 | test_loss: 0.1799 | test_acc: 0.9630\n",
      "Epoch: 7 | train_loss: 0.1895 | train_acc: 0.9432 | test_loss: 0.1631 | test_acc: 0.9641\n",
      "Epoch: 8 | train_loss: 0.1788 | train_acc: 0.9492 | test_loss: 0.1493 | test_acc: 0.9641\n",
      "Epoch: 9 | train_loss: 0.1648 | train_acc: 0.9553 | test_loss: 0.1390 | test_acc: 0.9757\n",
      "Epoch: 10 | train_loss: 0.1535 | train_acc: 0.9618 | test_loss: 0.1312 | test_acc: 0.9699\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 0}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138536690cab4f6f93f24563c2d8c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6437 | train_acc: 0.6905 | test_loss: 0.5201 | test_acc: 0.8912\n",
      "Epoch: 2 | train_loss: 0.4635 | train_acc: 0.9003 | test_loss: 0.3974 | test_acc: 0.9213\n",
      "Epoch: 3 | train_loss: 0.3770 | train_acc: 0.9086 | test_loss: 0.3321 | test_acc: 0.9236\n",
      "Epoch: 4 | train_loss: 0.3278 | train_acc: 0.9125 | test_loss: 0.2916 | test_acc: 0.9259\n",
      "Epoch: 5 | train_loss: 0.2959 | train_acc: 0.9138 | test_loss: 0.2639 | test_acc: 0.9248\n",
      "Epoch: 6 | train_loss: 0.2722 | train_acc: 0.9202 | test_loss: 0.2445 | test_acc: 0.9363\n",
      "Epoch: 7 | train_loss: 0.2543 | train_acc: 0.9225 | test_loss: 0.2282 | test_acc: 0.9387\n",
      "Epoch: 8 | train_loss: 0.2445 | train_acc: 0.9220 | test_loss: 0.2145 | test_acc: 0.9387\n",
      "Epoch: 9 | train_loss: 0.2304 | train_acc: 0.9248 | test_loss: 0.2041 | test_acc: 0.9375\n",
      "Epoch: 10 | train_loss: 0.2197 | train_acc: 0.9302 | test_loss: 0.1955 | test_acc: 0.9398\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539ace5e9db046fb846e9298158e743d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6437 | train_acc: 0.6905 | test_loss: 0.5201 | test_acc: 0.8912\n",
      "Epoch: 2 | train_loss: 0.4635 | train_acc: 0.9003 | test_loss: 0.3974 | test_acc: 0.9213\n",
      "Epoch: 3 | train_loss: 0.3770 | train_acc: 0.9086 | test_loss: 0.3321 | test_acc: 0.9236\n",
      "Epoch: 4 | train_loss: 0.3278 | train_acc: 0.9125 | test_loss: 0.2916 | test_acc: 0.9259\n",
      "Epoch: 5 | train_loss: 0.2959 | train_acc: 0.9138 | test_loss: 0.2639 | test_acc: 0.9248\n",
      "Epoch: 6 | train_loss: 0.2722 | train_acc: 0.9202 | test_loss: 0.2445 | test_acc: 0.9363\n",
      "Epoch: 7 | train_loss: 0.2543 | train_acc: 0.9225 | test_loss: 0.2282 | test_acc: 0.9387\n",
      "Epoch: 8 | train_loss: 0.2445 | train_acc: 0.9220 | test_loss: 0.2145 | test_acc: 0.9387\n",
      "Epoch: 9 | train_loss: 0.2304 | train_acc: 0.9248 | test_loss: 0.2041 | test_acc: 0.9375\n",
      "Epoch: 10 | train_loss: 0.2197 | train_acc: 0.9302 | test_loss: 0.1955 | test_acc: 0.9398\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61192a35d2384d63a42f5440592d16e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6626 | train_acc: 0.6630 | test_loss: 0.5632 | test_acc: 0.8873\n",
      "Epoch: 2 | train_loss: 0.5122 | train_acc: 0.8907 | test_loss: 0.4433 | test_acc: 0.9230\n",
      "Epoch: 3 | train_loss: 0.4191 | train_acc: 0.9082 | test_loss: 0.3653 | test_acc: 0.9263\n",
      "Epoch: 4 | train_loss: 0.3578 | train_acc: 0.9109 | test_loss: 0.3129 | test_acc: 0.9286\n",
      "Epoch: 5 | train_loss: 0.3152 | train_acc: 0.9172 | test_loss: 0.2761 | test_acc: 0.9308\n",
      "Epoch: 6 | train_loss: 0.2838 | train_acc: 0.9214 | test_loss: 0.2478 | test_acc: 0.9408\n",
      "Epoch: 7 | train_loss: 0.2585 | train_acc: 0.9250 | test_loss: 0.2267 | test_acc: 0.9531\n",
      "Epoch: 8 | train_loss: 0.2401 | train_acc: 0.9294 | test_loss: 0.2077 | test_acc: 0.9453\n",
      "Epoch: 9 | train_loss: 0.2231 | train_acc: 0.9368 | test_loss: 0.1939 | test_acc: 0.9453\n",
      "Epoch: 10 | train_loss: 0.2100 | train_acc: 0.9352 | test_loss: 0.1812 | test_acc: 0.9621\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3baef58099c4237873ae66f66790608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6626 | train_acc: 0.6630 | test_loss: 0.5632 | test_acc: 0.8873\n",
      "Epoch: 2 | train_loss: 0.5122 | train_acc: 0.8907 | test_loss: 0.4433 | test_acc: 0.9230\n",
      "Epoch: 3 | train_loss: 0.4191 | train_acc: 0.9082 | test_loss: 0.3653 | test_acc: 0.9263\n",
      "Epoch: 4 | train_loss: 0.3578 | train_acc: 0.9109 | test_loss: 0.3129 | test_acc: 0.9286\n",
      "Epoch: 5 | train_loss: 0.3152 | train_acc: 0.9172 | test_loss: 0.2761 | test_acc: 0.9308\n",
      "Epoch: 6 | train_loss: 0.2838 | train_acc: 0.9214 | test_loss: 0.2478 | test_acc: 0.9408\n",
      "Epoch: 7 | train_loss: 0.2585 | train_acc: 0.9250 | test_loss: 0.2267 | test_acc: 0.9531\n",
      "Epoch: 8 | train_loss: 0.2401 | train_acc: 0.9294 | test_loss: 0.2077 | test_acc: 0.9453\n",
      "Epoch: 9 | train_loss: 0.2231 | train_acc: 0.9368 | test_loss: 0.1939 | test_acc: 0.9453\n",
      "Epoch: 10 | train_loss: 0.2100 | train_acc: 0.9352 | test_loss: 0.1812 | test_acc: 0.9621\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 64, 'optimizer': 'SGD', 'weight_decay': 0}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9671758eb41648839ce7fb971df2c520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.7081 | train_acc: 0.4707 | test_loss: 0.6266 | test_acc: 0.6004\n",
      "Epoch: 2 | train_loss: 0.5784 | train_acc: 0.8524 | test_loss: 0.5167 | test_acc: 0.8996\n",
      "Epoch: 3 | train_loss: 0.4920 | train_acc: 0.8930 | test_loss: 0.4436 | test_acc: 0.9029\n",
      "Epoch: 4 | train_loss: 0.4340 | train_acc: 0.9022 | test_loss: 0.3935 | test_acc: 0.9241\n",
      "Epoch: 5 | train_loss: 0.3928 | train_acc: 0.9095 | test_loss: 0.3567 | test_acc: 0.9252\n",
      "Epoch: 6 | train_loss: 0.3619 | train_acc: 0.9104 | test_loss: 0.3279 | test_acc: 0.9275\n",
      "Epoch: 7 | train_loss: 0.3369 | train_acc: 0.9133 | test_loss: 0.3061 | test_acc: 0.9286\n",
      "Epoch: 8 | train_loss: 0.3188 | train_acc: 0.9116 | test_loss: 0.2874 | test_acc: 0.9286\n",
      "Epoch: 9 | train_loss: 0.3020 | train_acc: 0.9142 | test_loss: 0.2732 | test_acc: 0.9286\n",
      "Epoch: 10 | train_loss: 0.2888 | train_acc: 0.9173 | test_loss: 0.2598 | test_acc: 0.9342\n",
      "\n",
      "\u001b[1mTesting hyperparameters: {'learning_rate': 0.0001, 'batch_size': 64, 'optimizer': 'SGD', 'weight_decay': 0.0001}\u001b[0m\n",
      "Training with SGD optimizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ecb1224f8744b785d0d4c4dab5bb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.7081 | train_acc: 0.4707 | test_loss: 0.6266 | test_acc: 0.6004\n",
      "Epoch: 2 | train_loss: 0.5784 | train_acc: 0.8524 | test_loss: 0.5167 | test_acc: 0.8996\n",
      "Epoch: 3 | train_loss: 0.4920 | train_acc: 0.8930 | test_loss: 0.4436 | test_acc: 0.9029\n",
      "Epoch: 4 | train_loss: 0.4340 | train_acc: 0.9022 | test_loss: 0.3935 | test_acc: 0.9241\n",
      "Epoch: 5 | train_loss: 0.3928 | train_acc: 0.9095 | test_loss: 0.3567 | test_acc: 0.9252\n",
      "Epoch: 6 | train_loss: 0.3619 | train_acc: 0.9104 | test_loss: 0.3279 | test_acc: 0.9275\n",
      "Epoch: 7 | train_loss: 0.3369 | train_acc: 0.9133 | test_loss: 0.3061 | test_acc: 0.9286\n",
      "Epoch: 8 | train_loss: 0.3188 | train_acc: 0.9116 | test_loss: 0.2874 | test_acc: 0.9286\n",
      "Epoch: 9 | train_loss: 0.3021 | train_acc: 0.9142 | test_loss: 0.2732 | test_acc: 0.9286\n",
      "Epoch: 10 | train_loss: 0.2888 | train_acc: 0.9173 | test_loss: 0.2598 | test_acc: 0.9342\n",
      "\n",
      "\u001b[1mBest hyperparameters: {'learning_rate': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0}\n",
      "Best validation accuracy: 0.9955\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ec2e8f7134470ebacded730472e334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.30 GiB is allocated by PyTorch, and 178.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 147\u001b[0m\n\u001b[0;32m    144\u001b[0m best_model\u001b[38;5;241m.\u001b[39mload_state_dict(best_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Not used but required by function\u001b[39;49;00m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Not used in evaluation\u001b[39;49;00m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only evaluation\u001b[39;49;00m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1mFinal test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\ransom\\cnn2\\MORE\\final_project\\json to image\\ViT\\going_modular\\going_modular\\engine.py:169\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m--> 169\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    175\u001b[0m       dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m    176\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m    177\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\ransom\\cnn2\\MORE\\final_project\\json to image\\ViT\\going_modular\\going_modular\\engine.py:45\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     42\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[0;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\vision_transformer.py:298\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    295\u001b[0m batch_class_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_token\u001b[38;5;241m.\u001b[39mexpand(n, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    296\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_class_token, x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[0;32m    301\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\vision_transformer.py:157\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m torch\u001b[38;5;241m.\u001b[39m_assert(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\vision_transformer.py:118\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    117\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)\n\u001b[1;32m--> 118\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\activation.py:734\u001b[0m, in \u001b[0;36mGELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.30 GiB is allocated by PyTorch, and 178.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "from helper_functions import set_seeds\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Load pretrained ViT model\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# Freeze base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = r'C:\\Users\\satya\\Desktop\\ransom\\cnn2\\MORE\\RGBprocessed_images\\train'\n",
    "test_dir = r'C:\\Users\\satya\\Desktop\\ransom\\cnn2\\MORE\\RGBprocessed_images\\test'\n",
    "val_dir = r'C:\\Users\\satya\\Desktop\\ransom\\cnn2\\MORE\\RGBprocessed_images\\val'\n",
    "\n",
    "\n",
    "\n",
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "\n",
    "# Function to create dataloaders\n",
    "def create_dataloaders(train_dir, val_dir, test_dir, transform, batch_size, num_workers=os.cpu_count()):\n",
    "    # Create datasets\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    val_data = datasets.ImageFolder(val_dir, transform=transform)  # Validation dataset\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader, class_names\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparam_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4],  # Learning rates to test\n",
    "    'batch_size': [32, 64],         # Batch sizes to test\n",
    "    'optimizer': ['Adam', 'SGD'],   # Optimizers to test\n",
    "    'weight_decay': [0, 1e-4]       # Weight decay (regularization)\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "grid_combinations = [dict(zip(hyperparam_grid.keys(), values)) \n",
    "                     for values in product(*hyperparam_grid.values())]\n",
    "\n",
    "# Track best model and metrics\n",
    "best_metrics = {\n",
    "    'accuracy': 0,\n",
    "    'hyperparams': None,\n",
    "    'model_state': None\n",
    "}\n",
    "\n",
    "# Grid search loop\n",
    "for params in grid_combinations:\n",
    "    print(f\"\\n\\033[1mTesting hyperparameters: {params}\\033[0m\")\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    set_seeds()\n",
    "    \n",
    "    # Create dataloaders with current batch size\n",
    "    train_dataloader, val_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "        train_dir=train_dir,\n",
    "        val_dir=val_dir,\n",
    "        test_dir=test_dir,\n",
    "        transform=pretrained_vit_transforms,\n",
    "        batch_size=params['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Initialize fresh model for each combination\n",
    "    model = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    model.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "    \n",
    "    # Configure optimizer\n",
    "    optimizer_config = {\n",
    "        'params': model.parameters(),\n",
    "        'lr': params['learning_rate'],\n",
    "        'weight_decay': params['weight_decay']\n",
    "    }\n",
    "    if params['optimizer'] == 'Adam':\n",
    "        optimizer = torch.optim.Adam(**optimizer_config)\n",
    "    elif params['optimizer'] == 'SGD':\n",
    "        optimizer = torch.optim.SGD(**optimizer_config, momentum=0.9)\n",
    "    \n",
    "    # Train model with validation\n",
    "    from going_modular.going_modular import engine\n",
    "    print(f\"Training with {params['optimizer']} optimizer...\")\n",
    "    results = engine.train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=val_dataloader,  # Validate on validation set\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        epochs=10,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Track best model using validation accuracy\n",
    "    current_val_acc = max(results['test_acc'])\n",
    "    if current_val_acc > best_metrics['accuracy']:\n",
    "        best_metrics['accuracy'] = current_val_acc\n",
    "        best_metrics['hyperparams'] = params\n",
    "        best_metrics['model_state'] = model.state_dict()\n",
    "        print(f\"New best validation accuracy: {current_val_acc:.4f}\")\n",
    "\n",
    "# Final evaluation with best model\n",
    "print(\"\\n\\033[1mBest hyperparameters:\", best_metrics['hyperparams'])\n",
    "print(f\"Best validation accuracy: {best_metrics['accuracy']:.4f}\\033[0m\")\n",
    "\n",
    "# Load best model for test evaluation\n",
    "best_model = torchvision.models.vit_b_16().to(device)\n",
    "best_model.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "best_model.load_state_dict(best_metrics['model_state'])\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = engine.train(\n",
    "    model=best_model,\n",
    "    train_dataloader=train_dataloader,  # Not used but required by function\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,  # Not used in evaluation\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    epochs=1,  # Only evaluation\n",
    "    device=device\n",
    ")\n",
    "print(f\"\\n\\033[1mFinal test accuracy: {max(test_results['test_acc']):.4f}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ade9a4e-7224-42dd-8799-afd8990eb009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9b4275c1ff46b78972bcd026a29d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 141.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Not used but required by function\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Not used in evaluation\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only evaluation\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1mFinal test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\ransom\\cnn2\\MORE\\final_project\\json to image\\ViT\\going_modular\\going_modular\\engine.py:169\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m--> 169\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    175\u001b[0m       dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m    176\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m    177\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\ransom\\cnn2\\MORE\\final_project\\json to image\\ViT\\going_modular\\going_modular\\engine.py:42\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Loop through data loader data batches\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Send data to target device\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 141.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test set\n",
    "test_results = engine.train(\n",
    "    model=best_model,\n",
    "    train_dataloader=train_dataloader,  # Not used but required by function\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,  # Not used in evaluation\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    epochs=1,  # Only evaluation\n",
    "    device=device\n",
    ")\n",
    "print(f\"\\n\\033[1mFinal test accuracy: {max(test_results['test_acc']):.4f}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7817c3-531b-4f7f-a455-9aee5db8f7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
